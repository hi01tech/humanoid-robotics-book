"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[406],{5680(e,t,n){n.d(t,{xA:()=>d,yg:()=>m});var o=n(6540);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter(function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable})),n.push.apply(n,o)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach(function(t){r(e,t,n[t])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach(function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))})}return e}function s(e,t){if(null==e)return{};var n,o,r=function(e,t){if(null==e)return{};var n,o,r={},i=Object.keys(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=o.createContext({}),c=function(e){var t=o.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},d=function(e){var t=c(e.components);return o.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},p=o.forwardRef(function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),p=c(n),m=r,g=p["".concat(l,".").concat(m)]||p[m]||u[m]||i;return n?o.createElement(g,a(a({ref:t},d),{},{components:n})):o.createElement(g,a({ref:t},d))});function m(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,a=new Array(i);a[0]=p;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,a[1]=s;for(var c=2;c<i;c++)a[c]=n[c];return o.createElement.apply(null,a)}return o.createElement.apply(null,n)}p.displayName="MDXCreateElement"},8100(e,t,n){n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var o=n(8168),r=(n(6540),n(5680));const i={id:"vla-week11",title:"Week 11: Introduction to Vision-Language-Action (VLA) Models",sidebar_label:"Week 11: Intro to VLA",estimated_time:4,week:11,module:"VLA",prerequisites:["isaac-ros-detection"],learning_objectives:["Understand the concept of Vision-Language-Action models","Differentiate between VLMs and VLAs","Explore the architecture of prominent VLA models like Google's RT-2","Discuss the challenges and opportunities in VLA for robotics"]},a="Week 11: Introduction to Vision-Language-Action (VLA) Models",s={unversionedId:"vla/vla-week11",id:"vla/vla-week11",title:"Week 11: Introduction to Vision-Language-Action (VLA) Models",description:"This week marks our transition into one of the most exciting and rapidly developing areas in robotics: Vision-Language-Action (VLA) models. These models represent a paradigm shift, enabling robots to understand complex instructions, perceive their environment, and execute tasks using a unified framework.",source:"@site/docs/vla/week11.md",sourceDirName:"vla",slug:"/vla/vla-week11",permalink:"/humanoid-robotics-book/docs/vla/vla-week11",draft:!1,tags:[],version:"current",frontMatter:{id:"vla-week11",title:"Week 11: Introduction to Vision-Language-Action (VLA) Models",sidebar_label:"Week 11: Intro to VLA",estimated_time:4,week:11,module:"VLA",prerequisites:["isaac-ros-detection"],learning_objectives:["Understand the concept of Vision-Language-Action models","Differentiate between VLMs and VLAs","Explore the architecture of prominent VLA models like Google's RT-2","Discuss the challenges and opportunities in VLA for robotics"]},sidebar:"tutorialSidebar",previous:{title:"Module 4: VLA",permalink:"/humanoid-robotics-book/docs/vla-module"},next:{title:"Week 12: Action Tokenization",permalink:"/humanoid-robotics-book/docs/vla/vla-week12"}},l={},c=[{value:"What are VLA Models?",id:"what-are-vla-models",level:2},{value:"Prominent VLA Architectures",id:"prominent-vla-architectures",level:2}],d={toc:c};function u({components:e,...t}){return(0,r.yg)("wrapper",(0,o.A)({},d,t,{components:e,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"week-11-introduction-to-vision-language-action-vla-models"},"Week 11: Introduction to Vision-Language-Action (VLA) Models"),(0,r.yg)("p",null,"This week marks our transition into one of the most exciting and rapidly developing areas in robotics: ",(0,r.yg)("strong",{parentName:"p"},"Vision-Language-Action (VLA) models"),". These models represent a paradigm shift, enabling robots to understand complex instructions, perceive their environment, and execute tasks using a unified framework."),(0,r.yg)("h2",{id:"what-are-vla-models"},"What are VLA Models?"),(0,r.yg)("p",null,"VLA models are a type of AI that can process visual input (images, videos), understand natural language commands, and translate these into physical actions for a robot. They build upon ",(0,r.yg)("strong",{parentName:"p"},"Vision-Language Models (VLMs)"),", which focus on understanding the relationship between images and text, by adding the crucial element of ",(0,r.yg)("strong",{parentName:"p"},"Action"),"."),(0,r.yg)("h2",{id:"prominent-vla-architectures"},"Prominent VLA Architectures"),(0,r.yg)("p",null,"We will delve into the architecture of models like Google's ",(0,r.yg)("strong",{parentName:"p"},"RT-2 (Robotics Transformer 2)"),", which demonstrates how large language models can be adapted to control real-world robots by outputting action tokens."),(0,r.yg)("p",null,(0,r.yg)("em",{parentName:"p"},"(Content to be developed - will leverage NVIDIA GR00T and OpenVLA resources as researched)")))}u.isMDXComponent=!0}}]);