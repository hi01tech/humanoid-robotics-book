"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[668],{5680(e,n,t){t.d(n,{xA:()=>d,yg:()=>p});var o=t(6540);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),t.push.apply(t,o)}return t}function a(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach(function(n){i(e,n,t[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})}return e}function l(e,n){if(null==e)return{};var t,o,i=function(e,n){if(null==e)return{};var t,o,i={},r=Object.keys(e);for(o=0;o<r.length;o++)t=r[o],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)t=r[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var c=o.createContext({}),s=function(e){var n=o.useContext(c),t=n;return e&&(t="function"==typeof e?e(n):a(a({},n),e)),t},d=function(e){var n=s(e.components);return o.createElement(c.Provider,{value:n},e.children)},u={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},g=o.forwardRef(function(e,n){var t=e.components,i=e.mdxType,r=e.originalType,c=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),g=s(t),p=i,m=g["".concat(c,".").concat(p)]||g[p]||u[p]||r;return t?o.createElement(m,a(a({ref:n},d),{},{components:t})):o.createElement(m,a({ref:n},d))});function p(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var r=t.length,a=new Array(r);a[0]=g;var l={};for(var c in n)hasOwnProperty.call(n,c)&&(l[c]=n[c]);l.originalType=e,l.mdxType="string"==typeof e?e:i,a[1]=l;for(var s=2;s<r;s++)a[s]=t[s];return o.createElement.apply(null,a)}return o.createElement.apply(null,t)}g.displayName="MDXCreateElement"},8209(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>s});var o=t(8168),i=(t(6540),t(5680));const r={id:"vla-week12",title:"Week 12: Action Tokenization and Policy Learning",sidebar_label:"Week 12: Action Tokenization",estimated_time:4,week:12,module:"VLA",prerequisites:["vla-week11"],learning_objectives:["Understand how continuous robot actions are discretized into tokens","Explore various methods of action tokenization","Grasp the concepts of policy learning in the context of VLA models","Discuss the role of imitation learning and reinforcement learning in VLA training"]},a="Week 12: Action Tokenization and Policy Learning",l={unversionedId:"vla/vla-week12",id:"vla/vla-week12",title:"Week 12: Action Tokenization and Policy Learning",description:"This week, we will dive deeper into the mechanics of how VLA models generate robot movements. A key challenge in connecting high-level language commands and visual observations to low-level robot control is action tokenization.",source:"@site/docs/vla/week12.md",sourceDirName:"vla",slug:"/vla/vla-week12",permalink:"/humanoid-robotics-book/docs/vla/vla-week12",draft:!1,tags:[],version:"current",frontMatter:{id:"vla-week12",title:"Week 12: Action Tokenization and Policy Learning",sidebar_label:"Week 12: Action Tokenization",estimated_time:4,week:12,module:"VLA",prerequisites:["vla-week11"],learning_objectives:["Understand how continuous robot actions are discretized into tokens","Explore various methods of action tokenization","Grasp the concepts of policy learning in the context of VLA models","Discuss the role of imitation learning and reinforcement learning in VLA training"]},sidebar:"tutorialSidebar",previous:{title:"Week 11: Intro to VLA",permalink:"/humanoid-robotics-book/docs/vla/vla-week11"},next:{title:"Week 13: Advanced VLA",permalink:"/humanoid-robotics-book/docs/vla/vla-week13"}},c={},s=[{value:"Action Tokenization",id:"action-tokenization",level:2},{value:"Policy Learning",id:"policy-learning",level:2}],d={toc:s};function u({components:e,...n}){return(0,i.yg)("wrapper",(0,o.A)({},d,n,{components:e,mdxType:"MDXLayout"}),(0,i.yg)("h1",{id:"week-12-action-tokenization-and-policy-learning"},"Week 12: Action Tokenization and Policy Learning"),(0,i.yg)("p",null,"This week, we will dive deeper into the mechanics of how VLA models generate robot movements. A key challenge in connecting high-level language commands and visual observations to low-level robot control is ",(0,i.yg)("strong",{parentName:"p"},"action tokenization"),"."),(0,i.yg)("h2",{id:"action-tokenization"},"Action Tokenization"),(0,i.yg)("p",null,"Robot actions are typically continuous (e.g., joint angles, end-effector velocities). For VLA models that leverage transformer architectures (originally designed for discrete language tokens), these continuous actions must be converted into a discrete, tokenized format. We will explore different strategies for achieving this, including discretizing continuous spaces and predicting action primitives."),(0,i.yg)("h2",{id:"policy-learning"},"Policy Learning"),(0,i.yg)("p",null,"We will then investigate how VLA models learn to map observations and instructions to these action tokens. This involves various techniques from machine learning, particularly:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Imitation Learning"),": Training models by demonstrating desired behaviors."),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Reinforcement Learning"),": Learning through trial and error, guided by reward signals.")),(0,i.yg)("p",null,(0,i.yg)("em",{parentName:"p"},"(Content to be developed - will leverage NVIDIA GR00T and OpenVLA resources as researched)")))}u.isMDXComponent=!0}}]);