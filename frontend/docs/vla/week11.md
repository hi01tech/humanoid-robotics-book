---
id: vla-week11
title: "Week 11: Introduction to Vision-Language-Action (VLA) Models"
sidebar_label: "Week 11: Intro to VLA"
estimated_time: 4
week: 11
module: "VLA"
prerequisites:
  - "isaac-ros-detection"
learning_objectives:
  - "Understand the concept of Vision-Language-Action models"
  - "Differentiate between VLMs and VLAs"
  - "Explore the architecture of prominent VLA models like Google's RT-2"
  - "Discuss the challenges and opportunities in VLA for robotics"
---

# Week 11: Introduction to Vision-Language-Action (VLA) Models

This week marks our transition into one of the most exciting and rapidly developing areas in robotics: **Vision-Language-Action (VLA) models**. These models represent a paradigm shift, enabling robots to understand complex instructions, perceive their environment, and execute tasks using a unified framework.

## What are VLA Models?

VLA models are a type of AI that can process visual input (images, videos), understand natural language commands, and translate these into physical actions for a robot. They build upon **Vision-Language Models (VLMs)**, which focus on understanding the relationship between images and text, by adding the crucial element of **Action**.

## Prominent VLA Architectures

We will delve into the architecture of models like Google's **RT-2 (Robotics Transformer 2)**, which demonstrates how large language models can be adapted to control real-world robots by outputting action tokens.

*(Content to be developed - will leverage NVIDIA GR00T and OpenVLA resources as researched)*
