<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-vla/vla-week11" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.3">
<title data-rh="true">Week 11: Introduction to Vision-Language-Action Models | Physical AI &amp; Humanoid Robotics</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://hi01tech.github.io/docs/vla/week11"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Week 11: Introduction to Vision-Language-Action Models | Physical AI &amp; Humanoid Robotics"><meta data-rh="true" name="description" content="This week, we will explore the cutting-edge of AI in robotics: Vision-Language-Action (VLA) models. These are large, multi-modal models that can take instructions in natural language, perceive the world through vision, and generate actions for a robot to execute."><meta data-rh="true" property="og:description" content="This week, we will explore the cutting-edge of AI in robotics: Vision-Language-Action (VLA) models. These are large, multi-modal models that can take instructions in natural language, perceive the world through vision, and generate actions for a robot to execute."><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://hi01tech.github.io/docs/vla/week11"><link data-rh="true" rel="alternate" href="https://hi01tech.github.io/docs/vla/week11" hreflang="en"><link data-rh="true" rel="alternate" href="https://hi01tech.github.io/docs/vla/week11" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.c37c557d.css">
<link rel="preload" href="/assets/js/runtime~main.e393757d.js" as="script">
<link rel="preload" href="/assets/js/main.a2a40b15.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.png" alt="Logo" class="themedImage_ToTc themedImage--light_HNdA" height="50" width="50"><img src="/img/logo.png" alt="Logo" class="themedImage_ToTc themedImage--dark_i4oU" height="50" width="50"></div><b class="navbar__title text--truncate">Physical AI &amp; Humanoid Robotics</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/introduction-setup">Textbook</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/search">Search</a><a class="navbar__item navbar__link" href="/chatbot">Chatbot</a><div class="toggle_vylO colorModeToggle_x44X"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbar__item languageToggle_FqXF"><select class="languageSelect_ti5h"><option selected="" value="en">English</option><option value="ur">اردو</option></select></div> <div class="searchBox_H2mL"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/introduction-setup">Introduction &amp; Setup</a><button aria-label="Toggle the collapsible sidebar category &#x27;Introduction &amp; Setup&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/ros2-module">Module 1: ROS 2</a><button aria-label="Toggle the collapsible sidebar category &#x27;Module 1: ROS 2&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/digital-twin-module">Module 2: Digital Twin</a><button aria-label="Toggle the collapsible sidebar category &#x27;Module 2: Digital Twin&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/isaac-module">Module 3: NVIDIA Isaac</a><button aria-label="Toggle the collapsible sidebar category &#x27;Module 3: NVIDIA Isaac&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" href="/docs/vla-module">Module 4: VLA</a><button aria-label="Toggle the collapsible sidebar category &#x27;Module 4: VLA&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/vla/week11">Week 11: VLA Intro</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/vla/week12">Week 12: VLA with Isaac ROS</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/vla/week13">Week 13: The Future</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/assessments">Assessments</a><button aria-label="Toggle the collapsible sidebar category &#x27;Assessments&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/reference">Reference</a><button aria-label="Toggle the collapsible sidebar category &#x27;Reference&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/vla-module"><span itemprop="name">Module 4: VLA</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Week 11: VLA Intro</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Week 11: Introduction to Vision-Language-Action Models</h1><p>This week, we will explore the cutting-edge of AI in robotics: Vision-Language-Action (VLA) models. These are large, multi-modal models that can take instructions in natural language, perceive the world through vision, and generate actions for a robot to execute.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="topics-covered">Topics Covered<a href="#topics-covered" class="hash-link" aria-label="Direct link to Topics Covered" title="Direct link to Topics Covered">​</a></h2><ul><li><strong>What is a VLA?</strong> A deep dive into the concept and its implications.</li><li><strong>VLA Architectures:</strong> From CLIP to RT-2 and beyond.</li><li><strong>The Role of Foundation Models:</strong> How large pre-trained models are changing robotics.</li><li><strong>Fine-tuning and Adaptation:</strong> How to adapt a VLA to a new robot or task.</li></ul><p>This week, we will explore the cutting-edge of AI in robotics: Vision-Language-Action (VLA) models. These are large, multi-modal models that can take instructions in natural language, perceive the world through vision, and generate actions for a robot to execute.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-vla">What is a VLA?<a href="#what-is-a-vla" class="hash-link" aria-label="Direct link to What is a VLA?" title="Direct link to What is a VLA?">​</a></h2><p>A <strong>Vision-Language-Action (VLA) model</strong> is a type of AI model that integrates capabilities from computer vision, natural language understanding, and robot control. The core idea is to enable robots to understand and execute tasks described in human language, by grounding that language in visual perception and translating it into physical actions.</p><p>Imagine telling a robot, &quot;Pick up the red apple from the table and put it in the basket.&quot; A VLA model would:</p><ol><li><strong>Vision:</strong> Use its visual sensors (cameras) to identify the &quot;red apple&quot; and the &quot;basket&quot; on the &quot;table.&quot;</li><li><strong>Language:</strong> Understand the instruction &quot;Pick up... and put it in...&quot;</li><li><strong>Action:</strong> Generate a sequence of robot movements (e.g., approach the table, extend arm, grasp apple, move to basket, release apple) to fulfill the command.</li></ol><p>VLAs are a significant step towards more intuitive and capable human-robot interaction, moving beyond pre-programmed behaviors to truly intelligent and adaptable agents.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="vla-architectures">VLA Architectures<a href="#vla-architectures" class="hash-link" aria-label="Direct link to VLA Architectures" title="Direct link to VLA Architectures">​</a></h2><p>VLA models are often built upon powerful <strong>foundation models</strong> that have been pre-trained on massive datasets. Here are some key architectural concepts and examples:</p><ul><li><strong>Encoder-Decoder Structures:</strong> Many VLAs utilize an encoder-decoder architecture.<ul><li><strong>Vision Encoder:</strong> Processes visual input (images, video) into a rich representation (e.g., using a Vision Transformer like ViT or a ResNet).</li><li><strong>Language Encoder:</strong> Processes natural language instructions into a contextualized representation (e.g., using a Transformer like BERT or GPT).</li><li><strong>Action Decoder:</strong> Takes the combined visual and linguistic representations and translates them into a sequence of robot actions (e.g., joint commands, end-effector poses).</li></ul></li><li><strong>CLIP (Contrastive Language-Image Pre-training):</strong> While not a VLA itself, CLIP is a powerful foundation model that learns to associate text with images. It can determine if a given text description matches an image. VLAs often leverage CLIP&#x27;s understanding of visual concepts and their linguistic labels for tasks like object recognition or scene understanding based on natural language queries.</li><li><strong>RT-1 and RT-2 (Robotics Transformer 1 &amp; 2):</strong> Developed by Google DeepMind, these are prominent examples of end-to-end VLA models.<ul><li><strong>RT-1:</strong> Translates visual observations and language instructions directly into low-level joint commands. It was trained on a large dataset of real-world robot demonstrations.</li><li><strong>RT-2:</strong> Builds upon RT-1 by leveraging large Vision-Language Models (VLMs) as the core. It directly outputs robot actions as sequences of tokens, demonstrating impressive generalization capabilities from internet data to robot control.</li></ul></li><li><strong>Other Approaches:</strong> Research is active in many areas, including:<ul><li><strong>Diffusion Models for Action Generation:</strong> Using diffusion models to generate diverse and plausible robot trajectories.</li><li><strong>Embodied AI Agents:</strong> Integrating VLAs into agents that can learn and act in complex simulated or real-world environments.</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-role-of-foundation-models">The Role of Foundation Models<a href="#the-role-of-foundation-models" class="hash-link" aria-label="Direct link to The Role of Foundation Models" title="Direct link to The Role of Foundation Models">​</a></h2><p>Foundation models, such as large language models (LLMs) and large vision models (LVMs), have dramatically changed the landscape of AI. Their impact extends to robotics through VLAs.</p><ul><li><strong>Pre-trained Knowledge:</strong> Foundation models are trained on vast datasets, allowing them to acquire a broad understanding of language, vision, and often common-sense knowledge. This pre-trained knowledge is invaluable for VLAs, as it reduces the need for extensive task-specific training data.</li><li><strong>Generalization:</strong> By leveraging the generalization capabilities of foundation models, VLAs can often perform novel tasks or adapt to new environments with limited additional training.</li><li><strong>Multi-modal Understanding:</strong> Foundation models that combine vision and language (e.g., GPT-4V, Gemini) provide VLAs with a powerful ability to interpret complex instructions that refer to visual elements.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="fine-tuning-and-adaptation">Fine-tuning and Adaptation<a href="#fine-tuning-and-adaptation" class="hash-link" aria-label="Direct link to Fine-tuning and Adaptation" title="Direct link to Fine-tuning and Adaptation">​</a></h2><p>While foundation models provide a strong starting point, VLAs often require <strong>fine-tuning</strong> and adaptation to perform specific robotic tasks effectively.</p><ul><li><strong>Domain-Specific Data:</strong> Robots operate in physical environments with unique dynamics and constraints. Fine-tuning a VLA with real-world or high-fidelity simulated robot data (e.g., human demonstrations) helps it learn the nuances of robotic control.</li><li><strong>Task-Specific Adaptation:</strong> A general-purpose VLA might need to be adapted for specialized tasks, such as precise manipulation, interaction with deformable objects, or operating in confined spaces. This can involve further training on targeted datasets or using techniques like reinforcement learning.</li><li><strong>Prompt Engineering:</strong> For VLAs that rely on language instructions, careful <strong>prompt engineering</strong> can significantly improve performance. Crafting clear, concise, and context-rich prompts helps the model better understand the desired action.</li><li><strong>Embodied Feedback:</strong> Integrating feedback from the robot&#x27;s environment (e.g., tactile sensors, force sensors) during fine-tuning can help VLAs learn more robust and compliant behaviors.</li></ul></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/vla-module"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Module 4: VLA</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/vla/week12"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Week 12: VLA with Isaac ROS</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#topics-covered" class="table-of-contents__link toc-highlight">Topics Covered</a></li><li><a href="#what-is-a-vla" class="table-of-contents__link toc-highlight">What is a VLA?</a></li><li><a href="#vla-architectures" class="table-of-contents__link toc-highlight">VLA Architectures</a></li><li><a href="#the-role-of-foundation-models" class="table-of-contents__link toc-highlight">The Role of Foundation Models</a></li><li><a href="#fine-tuning-and-adaptation" class="table-of-contents__link toc-highlight">Fine-tuning and Adaptation</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Physical AI & Humanoid Robotics Textbook. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.e393757d.js"></script>
<script src="/assets/js/main.a2a40b15.js"></script>
</body>
</html>